

#Big O notation


Big O notation is used as a sort of measurement unit that helps programmers evaluate or estimate the efficiency of a written block of code, a script or an algorithm.

Itâ€™s hard to determine the exact runtime of a script or an algorithm. Which is also dependent on other factors such as the speed of the processor and other specifications of the computer in which that script or algorithm is running. So instead of evaluating the runtime directly, big O notation is used to evaluate how quickly the runtime grows relative to the input data to be processed by that algorithm.

Game: Date Transfer Speed by Internet vs Pigeon in 2009
















Ref: https://blog.bitsrc.io/algorithms-efficiency-big-o-in-simple-english-adbaedbcdfcf